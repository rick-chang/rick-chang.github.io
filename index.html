<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Jen-Hao Rick Chang</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/grayscale.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">
                    <span class="light">Jen-Hao Rick Chang</span>
                </a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">About</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#research">Research</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Intro Header -->
    <header class="intro">
        <div class="intro-body">
            <div class="container">
                <div class="row">
                    <div class="col-md-8 col-md-offset-2">
                        <h1 class="brand-heading">Rick Chang</h1>
                        <!-- <p class="intro-text">Play hard, work harder.</p> -->
                        <a href="#about" class="btn btn-circle page-scroll">
                            <i class="fa fa-angle-double-down animated"></i>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- About Section -->
    <section id="about" class="container content-section ">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2>About me</h2>
            </div>
        </div>
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <p>I am a research scientist at Apple. My research spans across multiple directions, including nerural rendering, generative models for sequences, and computational imaging and displays. My research has been incorporated into various Apple products, e.g., Siri, Scribble on iPad, QuickPath keyboard on iOS and watchOS. My current research interests are broadly in generative models, computational photography, and novel sensors.</p>

                <p>Before joining Apple in 2020, I received my PhD from the ECE department of Carnegie Mellon University, where I worked with professor <a href="https://users.ece.cmu.edu/~kumar/">Vijayakumar Bhagavatula</a> and professor <a href="https://users.ece.cmu.edu/~saswin/">Aswin Sankaranarayanan</a> on computational photography and displays, computer vision, and machine learning. I received my M.S and B.S degrees in electrical engineering from National Taiwan University working with professor <a href="http://teilab.ee.ntu.edu.tw/#/home">Tian-Li Yu</a>, and I worked with Dr. <a href="http://mml.citi.sinica.edu.tw">Yu-Chiang Frank Wang</a> on image processing as a research assistant.</p>
            </div>
        </div>
    </section>


    <!-- Portfolio Row Section -->
    <section id="research" class="content-section">
        <div class="container portfolio_row_container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Research</h2>
                </div>
            </div>

            <div class="row color_row">
                <div class="col-md-4 portfolio-item">
                    <a href="#portfolioModal_crf" class="portfolio-link" data-toggle="modal">
                        <video src="https://ml-site.cdn-apple.com/datasets/3d-shape-tokenization/objaverse_st_video_1x1.mp4" autoplay muted loop playsinline class="img-responsive"></video>
                    </a>  
                </div>
                <div class="col-md-8 portfolio-row">
                    <div class="portfolio-text">
                        <h4>3D Shape Tokenization with Flow Matching</h4>
                        We learn a 3D shape representation that is versitile, efficient, and easy to learn. The representation is learned by treating 3D surfaces as probabilistic density functions in 3D and utilizing flow matching to make the density function align with the 3D surfaces. We show that the learned representation can be used in a wide range of applications, including single-image to 3D, zero-shot 3D classification, and neural rendering of normal and depth maps. On all tasks, models based on our representation shows strong performance compared to individual state-of-the-art of each task.  
                        <p> 
                        <a href="https://machinelearning.apple.com/research/3d-shape-tokenization">paper and more info</a>,  
                        </p>
                    </div>    
                </div>  
            </div> 

            <div class="row color_row">
                <div class="col-md-4 portfolio-item">
                    <a href="#portfolioModal_crf" class="portfolio-link" data-toggle="modal">
                        <img src="img/research/nvas3d/nvas3d.png" class="img-responsive" alt="">
                    </a>  
                </div>
                <div class="col-md-8 portfolio-row">
                    <div class="portfolio-text">
                        <h4>Novel view acoustic synthesis</h4>
                        Novel-view synthesis of images is already a difficult problem; how about novel-view synthesis of sound? The "acoustic camera", i.e., a microphone, has a much lower resolution than photo cameras, and it captures sound from all sound sources, which are echoed differently from the scene. It is a very challenging problem. In this project, we demonstrate we are able to reconstruct the acoustic scene and synthesize spatial audio at any location in a 3d reconstructed scene.
                        <p> 
                        <a href="https://docs-assets.developer.apple.com/ml-research/models/nvas/nvas3d_turn.mp4">demo video (with headphone)</a>,  
                        <a href="https://github.com/apple/ml-nvas3d">code and model</a> 
                        </p>
                    </div>    
                </div>  
            </div> 

            <div class="row color_row">
                <div class="col-md-4 portfolio-item">
                    <a href="#portfolioModal_crf" class="portfolio-link" data-toggle="modal">
                        <img src="img/research/pointersect/pointersect_preview.png" class="img-responsive" alt="">
                    </a>  
                </div>
                <div class="col-md-8 portfolio-row">
                    <div class="portfolio-text">
                        <h4>Ray tracining on point clouds</h4>
                        Can we directly perform novel-view synthesis on a scene captured by a depth camera directly without any per-scene optimization? How do we perform ray tracing on a scene represented by point clouds?  In this project, we develop a first step toward the goal.
                        <p> 
                        <a href="https://machinelearning.apple.com/research/pointersect">CVPR 2023</a>, 
                        <a href="https://github.com/apple/ml-pointersect">code and model</a> 
                        </p>
                    </div>    
                </div>  
            </div> 

            <div class="row color_row">
                <div class="col-md-4 portfolio-item">
                    <a href="#portfolioModal_crf" class="portfolio-link" data-toggle="modal">
                        <img src="img/research/style_equalization/se.svg" class="img-responsive" alt="">
                    </a>  
                </div>
                <div class="col-md-8 portfolio-row">
                    <div class="portfolio-text">
                        <h4>Controllable Generative Models</h4>
                        Can generative models improve recognizers? In this line of works, we develop novel controllable generative models for sequence data (e.g., speech and handwriting) that generates realistic samples, and we demonstrate conditions under which synthetic data significantly benefit downstream recognizers. 
                        <p> <br>
                        Papers accepted by <br> ICML 2022 (<a href="https://apple.github.io/ml-style-equalization/">paper 1</a>) <br> ICASSP 2022 (<a href="https://arxiv.org/abs/2110.07040">paper 2</a>, <a href="https://arxiv.org/abs/2110.11479">paper 3</a>)
                        </p>
                    </div>    
                </div>  
            </div> 
            <div class="row color_row">
                <div class="col-md-4 portfolio-item">
                    <a href="#portfolioModal_crf" class="portfolio-link" data-toggle="modal">
                        <img src="img/research/token_pooling/token_pooling_vs_deit.svg" class="img-responsive" alt="">
                    </a>  
                </div>
                <div class="col-md-8 portfolio-row">
                    <div class="portfolio-text">
                        <h4>Token Pooling for Vision Transformers</h4>
                        We propose an efficient pooling (token downsampling) algorithm for transformer models. Our analysis shows that softmax attention is a high-dimensional low-pass filter on the input tokens. This means every attention layer produces redundant information. Based on our analysis, we design an algorithm that carefully prunes the redundancy. Simply by inserting our Token Pooling layer after each attention layer, we achieve same accuracy as our baseline models with 40% improvement in compuration cost on ImageNet-1k.
                        <p> <br>
                        <a href="https://arxiv.org/abs/2110.03860">paper</a>
                        </p>
                    </div>    
                </div>  
            </div> 
            <div class="row color_row">
                <div class="col-md-4 portfolio-item">
                    <a href="#portfolioModal_crf" class="portfolio-link" data-toggle="modal">
                        <img src="img/research/occlusion_aware/square_head.gif" class="img-responsive" alt="">
                    </a>  
                </div>
                <div class="col-md-8 portfolio-row">
                    <div class="portfolio-text">
                        <h4>Occlusion-aware Multifocal Displays</h4>
                        Multifocal displays alleviate vergence-accommodation conflict by simultaneously showing contents on transparent focal planes placed at different depths in front of our eyes. Despite its effectiveness to create the accommodation cue, the solution weakens the occlusion cue, because any small movement of our eyes can easily cause content shown on the different focal planes to overlap. The inability for the transparent focal planes to occlude light also significantly reduces the contrast of the virtual world. In the work, we enable occlusion-aware multifocal displays by enabling each display pixels to tilt light. Using a lab prototype, we demonstrate the presence of occlusion cues as well as the increase in the contrast of the display on a range of scenes.
                        <p> <br>
                        Accepted by SIGGRAPH 2020 (<a href="https://youtu.be/9NRuhg3_GGI">intro video</a>, <a href="img/research/occlusion_aware/conetilt.pdf">paper</a>, <a href="img/research/occlusion_aware/supplemental.pdf">supplementary</a>)
                        </p>
                    </div>    
                </div>  
            </div> 
            <div class="row color_row">
                <div class="col-md-4 portfolio-item">
                    <a href="#portfolioModal_crf" class="portfolio-link" data-toggle="modal">
                        <!-- <div class="caption">
                            <div class="caption-content">
                                <i class="fa fa-search-plus fa-3x"></i>
                            </div>
                        </div>
                        -->
                        <img src="img/research/dense_focal/focal_stack.gif" class="img-responsive" alt="">
                    </a>  
                </div>
                <div class="col-md-8 portfolio-row">
                    <div class="portfolio-text">
                        <h4>Toward Multifocal Displays with Dense Focal Stacks</h4>
                        Modern virtual reality displays suffer from the vergence-accommodation conflict, which causes discomfort and fatigue after long duration of usages.  While multifocal displays can alleviate the problem, they often have low retinal resolution due to the limited number of focal planes they can generate in a second.  In the paper, we present a first-of-its-kind multifocal display that is capable of generating a dense collection of focal planes (an order of magnitude greater in number as compared to prior work).  Operating at 1600 focal planes per second, our lab prototype can produce 3D cues even for a single eye and thereby is capable of resolving the vergence-accommodation conflict endemic to todayâ€™s VR displays. 
                        <p> <br>
                        Accepted by SIGGRAPH Asia 2018 (<a href="https://youtu.be/BmdSNirmjUU">intro video</a>, <a href="img/research/dense_focal/dense_focal_stacks.pdf">paper</a>, <a href="img/research/dense_focal/supplemental.pdf">supplementary</a>)
                        </p>
                    </div>    
                </div>  
            </div> 
            <div class="row color_row">
                <div class="col-md-4 portfolio-item">
                    <a href="#portfolioModal_crf" class="portfolio-link" data-toggle="modal">
                        <!-- <div class="caption">
                            <div class="caption-content">
                                <i class="fa fa-search-plus fa-3x"></i>
                            </div>
                        </div>
                        -->
                        <img src="img/research/ring/ring.png" class="img-responsive" alt="">
                    </a>  
                </div>
                <div class="col-md-8 portfolio-row">
                    <div class="portfolio-text">
                        <h4>One Network to Solve Them All</h4>
                        A hallmark of state-of-the-art algorithms for linear inverse problems is to train different neural nets for different problems.  This becomes very inefficient if we want to incorporate these specially-trained networks into portable devices like cell phones.   Cell phone cameras need to deal with a variety of image processing problems, from image deblurring, demosaicing, to super-resolution.  To achieve this, we propose a framework that can use a <em>single</em> network to solve <em>any</em> linear inverse problem.  This can significantly reduce the cost and complexity of the image signal processor.
                        <p> <br>
                        Accepted by ICCV 2017 (oral presentation) (<a href="https://youtu.be/s_r53HZRbIQ">intro video</a>, <a href="img/research/ring/deep_projection.pdf">paper</a>, <a href="https://github.com/rick-chang/OneNet">code</a>)
                        </p>
                    </div>    
                </div>  
            </div> 
            <div class="row color_row">
                <div class="col-md-4 portfolio-item">
                    <a href="#portfolioModal_crf" class="portfolio-link" data-toggle="modal">
                        <!-- <div class="caption">
                            <div class="caption-content">
                                <i class="fa fa-search-plus fa-3x"></i>
                            </div>
                        </div>
                        -->
                        <img src="img/research/random_feature/accuracy_vs_time.png" class="img-responsive" alt="">
                    </a>  
                </div>
                <div class="col-md-8 portfolio-row">
                    <div class="portfolio-text">
                        <h4>Random feature for sparse signal classification</h4>
                        Despite its ability to approximate almost any decision boundaries, traditional kernel method is difficult to use with large datasets, due to the high memory and computational requirements in both training and test phases. Recent works on random feature have demonstrated promising results of scalable kernel method. In this work, we provide theoretical analyses for random feature on a special yet commonly seen signal class --- sparse signals. It is known that image, video, and speech signals enjoy sparse representations after proper transformation. Our results provide tightened theoretical guarantees for this kind of signals. We also propose compressive random feature, which exploits signal sparsity to reduce data acquisition, memory, and computation requirements for nonlinear classification methods in both training and test phases.
                        <p> <br>
                        Accepted by CVPR 2016 (spotlight presentation). (<a href="img/research/random_feature/random_feature_paper.pdf">paper</a>, <a href="img/research/random_feature/random_feature_code.zip">code</a>)
                        </p>
                    </div>    
                </div>  
            </div> 
            <div class="row color_row">
                <div class="col-md-4 portfolio-item">
                    <a href="#portfolioModal_crf" class="portfolio-link" data-toggle="modal">
                        <!-- <div class="caption">
                            <div class="caption-content">
                                <i class="fa fa-search-plus fa-3x"></i>
                            </div>
                        </div>
                        -->
                        <img src="img/research/gray/gray.png" class="img-responsive" alt="">
                    </a>  
                </div>
                <div class="col-md-8 portfolio-row">
                    <div class="portfolio-text">
                        <h4>2<sup>16</sup> Shades of Gray --- High Bit-Depth Projector</h4>
                        While existing projection techniques work well for projecting low bit-depth (8-bit) images, it becomes infeasible for high bit-depth (say, 16-bit) projection --- a capability that is increasingly desirable in many applications including cinemas and gaming. In this paper, we describe a technique for high bit-depth projection using a single light modulator by adopting intensity-modulated light sources. The proposed design involves only a minor modification to traditional projector designs, namely intensity modulation of the light sources, and hence, can be adopted widely by both traditional low bit-depth projectors and modern high dynamic-range projectors. Finally, we present a prototype to showcase and validate the performance of the proposed design.
                        <p> <br>
                        Accepted by Optics Express, 2016. (<a href="img/research/gray/HDRProjector_OpEx16.pdf">paper</a>)
                        </p>
                    </div>    
                </div>  
            </div>
            <div class="row color_row">
                <div class="col-md-4 portfolio-item">
                    <a href="#portfolioModal_pfilter" class="portfolio-link" data-toggle="modal">
                        <img src="img/research/pfilter/pfilter_teaser.jpg" class="img-responsive" alt="">
                    </a>  
                </div>
                <div class="col-md-8 portfolio-row">
                    <div class="portfolio-text">
                        <h4>Propagated image filtering </h4>
                        When smoothing an image, we usually want to eliminate noise or wrinkles without losing image contexts (e.g., edges, textures, or important details).<br>
                        Our propagation image filters are designed exactly for this purpose --- context preserving image smoothing. 
                        <p> <br>
                        Accepted by CVPR 2015.  (<a href="img/research/pfilter/pfilter_paper.pdf">paper</a>, <a href="img/research/pfilter/pfilter_sup.pdf">supplement</a>, <a href="img/research/pfilter/pfilter_poster.pdf">poster</a>, <a href="img/research/pfilter/pfilter_code.zip">code</a>) 
                        </p>
                    </div>    
                </div>  
            </div> 
            <div class="row color_row">
                <div class="col-md-4 portfolio-item">
                    <a href="#portfolioModal_pfilter" class="portfolio-link" data-toggle="modal">
                        <!-- <div class="caption">
                            <div class="caption-content">
                                <i class="fa fa-search-plus fa-3x"></i>
                            </div>
                        </div>
                        -->
                        <img src="img/research/master_thesis/research_kr.png" class="img-responsive" alt="">
                    </a>  
                </div>
                <div class="col-md-8 portfolio-row">
                    <div class="portfolio-text">
                        <h4>Research on a Learning Model Presuming Representationalism, Functionalism, and Neural Darwinism</h4>
                        The human mind is a fascinating learning machine, which can solve a variety of problems with slightest prior knowledge. 
                        While neural network methods have demonstrated huge improvement on problem solving, the mystery of human mind is still waiting to be unraveled. Here, I study the human mind from a different angle by utilizing three complementary hypotheses about the human mind from cognitive science.  Specifically, I design and analyze a learning system that mimics human mind with context-free language.<br>
                        <p> <br>
                        This is my M.S. thesis.  The short version is <a href="img/research/master_thesis/thesis-preprint.pdf">here</a>.
                        The slides are <a href="img/research/master_thesis/msthesis_slides.pdf">here</a>.
                        </p>
                    </div>    
                </div>  
            </div> 
            <div class="row color_row">
                <div class="col-md-4 portfolio-item">
                    <a href="#portfolioModal_overlapping" class="portfolio-link" data-toggle="modal">
                        <!-- <div class="caption">
                            <div class="caption-content">
                                <i class="fa fa-search-plus fa-3x"></i>
                            </div>
                        </div>
                        -->
                        <img src="img/research/ga_test/test_function.png" class="img-responsive" alt="">
                    </a>  
                </div>
                <div class="col-md-8 portfolio-row">
                    <div class="portfolio-text">
                        <h4>A Test Function with Full Controllability Over Overlapping</h4>
                        While genetic algorithms work well on simple non-convex problems, it has been shown that they may fail on problems which contains complex inter-dependencies between variables.<br>
                        Our work provides an useful tool for analyzing the performance of genetic algorithms on this kind of complex problem. Specifically, we design a standardized test function to generate complex problems with controlled variable dependencies.
                        <p> <br>
                        Accepted by ACM Genetic and Evolutionary Computation Conference (GECCO) 2011 (<a href="img/research/ga_test/Technical Report.pdf">pdf</a>).
                        </p>
                    </div>    
                </div>  
            </div> 
            <div class="row color_row">
                <div class="col-md-10 col-md-offset-1">
                    <h3>Other projects</h3>
                    <h4>Dependency-Constrained Earth Movers' Distance</h4>
                    <p>The earth movers' distance measures the distance between two sets of points or histograms by solving the transportation problem.  Suppose there are cargoes to transport from source nodes to target nodes.
                    Given the weights of these cargoes and the distances between all source-target pairs.  The earth movers' distance calculates the minimum cost to transport these cargoes.  However, the earth movers' distance ignores the dependencies among these cargoes or points, and related source points may be transported to unrelated target points. We tackle the transportation problem with cargo dependencies and provide more accurate distance measurement when data dependencies exist. We use the proposed more accurate distance metric to improve computer vision applications, including image retrieval, color transfer. </p>
                    <h4>Message Passing Model Building Genetic Algorithm</h4> 
                    <p>The simple genetic algorithm can be easily paralleled, because all of its operators are local and not dependent on the whole population of candidate solutions.  However, when it comes to model building genetic algorithms, identifying variable dependencies from candidate solutions is not a local operator and needs to use information from the whole population.  This makes paralleled model building genetic algorithms difficult. In this work, we implement a distributed model building genetic algorithm, in which the model building process is paralleled by exchanging the distribution of each node's population.</p>

                    <h4>Association Rule Mining Genetic Algorithm</h4>
                    <p>We mine association rules from the population of candidate solutions.  These association rules are used to preserve high-fitness subsolutions.</p>

                    <h4>Rubik's Cube Solving Robot</h4>
                    <p>We implement a LEGO NXT robot, which can solve the Rubik's cube all by itself, including scanning the cube and preforming actions to the cube.</p>

                    <h4>Pitch and Tempo Detecting FPGA</h4>
                    <p>We realize a FPGA, which detects the pitch and tempo of a music.  This work earned the outstanding paper of the InnovateAsia FPGA Workshop and Design Contest 2009.</p>

                    <h4>Electromyography Controlled Toy Race Car</h4>
                    <p>We realize a circuit, which detects electromyography (EMG) signals and uses the signals to control a toy car :D</p>

                    <h4>Store Information Providing iPhone App</h4>
                    <p>We build an iPhone app, in which users can take photos of store's signboards to retrieve information about these stores.  We used SIFT as a feature to perform the recognition.<p>

                    <h4>Rule Evolving Agent for the Wumpus World</h4>
                    <p>We design a rule driven agent for the Wumpus world.  We study the process of how the agent adapts its belief toward its rules when encounters different situations.</p>

                    <h4>Facial Expression Recognition by Hierarchical Self-Organizing Map</h4>
                    <p>We design a two-level self organizing map to rocognize facial expressions.  This algorithm characterizes these expressions into both high-level and low-level terms.</p>

                    <h4>Smart Drug Box</h4>
                    <p>We design and implement the circuit of a smart drug box, which detects whether the drugs have been taken in order to notify patients.</p>

                </div>
        </div>
        </div>
    </section>




    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="js/jquery.easing.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/grayscale.js"></script>

</body>

</html>
